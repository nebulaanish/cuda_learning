
# Day 6
Writing code on CPU and GPU is the same. But understanding, how threads on GPU are scheduled, how they access memory, and how their execution proceeds can help write kernels that maximize the resources in hand. 

### Thread Hierarchy

- threads -> thread blocks -> thread cluster (Optional) -> Grids 
- Instrinsics like `gridDim.x|y|z` to query size fo the grid, `blockDim.x|y|z` to query block dimension. The index of thread block using `blockIdx.x|y|z` and thread index using `threadIdx.x|y|z`.  (thread dimension and block dimension are passed during kernel launch via execution configuration)

## Memory Spaces in GPU
![alt text](assets/image.png)

### Global Memory
Accessible to all threads in a grid and is the only way for numerical computations performed by GPU kernel to be transferred to the host(CPU).
- Allocated by CUDA Api calls like `cudaMalloc` and `cudaMallocManaged`. 
- Copy to host by using `cudaMemcpy`.
- Freed using `cudaFree`. 
- Global memory is persistent. So it must be removed manually, or termination of application. `cudaDeviceReset` deallocates entire global memory. 

### Shared Memory
Memory space, that's accessible to all threads in a thread block. 
- Persists throughout the kernel execution. 
- Small in size than global memory, is in each SM, high bandwidth, low latency. 
- Data race can occur between threads in same thread block. 
- Example code to avoid race condition. The data is read only after all threads are done writing in the `shared_data`.

```cpp
__global__ void syncthreadsexample(int* input_data, int* output_data){
    __shared__ int shared_data[128];  // assuming blockDim.x is 128
    // thread indices now range from 0 to blockDim.x-1. So shared_data can hold all of it. 
    shared_data[threadIdx.x] = input_data[threadIdx.x]; 

    __syncthreads();

    if (threadIdx.x==0){
        int sum = 0;
        for (int i=0, i<blockDim.x; ++i){
            sum += shared_data[i];
        }
        output_data[blockIdx.x] = sum;
    }

}
```

- Shared memory and L1 cache use the same physical space. The amount available varies by architecture. But, using more shared memory, reduces available L1 cache. 
- CUDA provides API to get query shared memory size on a per SM basis and per thread basis. 
- Also provides api to tell runtime whether to allocate more space to shared memory or L1 cache. It's specified during runtime, but no guarantee that it will be honored. 
- Shared memory allocation can be static or dynamic. 

**Static Memory allocation** : Can be done using `__shared__` inside the kernel. The lifetime will be for entire kernel. 
```cpp
__shared__ float my_array[1024];
```

**Dyanmic Memory Allocation**:
- Memory in bytes per thread block can be specified using a third and optional parameter: `functionName<<<grid, block, sharedMemoryBytes>>>()`
- Then using `extern __shared__` specifier to declare a variable. 
eg: 
```cpp
extern __shared__ float array[];
```
To allocated more than one dynamically allocated array, one needs to partition the initial dynamic array appropriately using pointer arithmetics. 

eg: 
to allocate the following: 
```cpp
short array0[128];
float array1[64];
int array2[256];
```
would translate to 

```cpp
extern __shared__ float array[];

short* array0 = (short*)array;  // starts at byte 0
float* array1 = (float*)&array0[128]; // skip 128 * size_of_short(2bytes)
int* array2 = (int*)&array1[64]; // skip 64 * size_of_float(4 bytes)
``` 


### Registers
- Located at SM and used by thread as local storage during execution of a kernel. 
- The number of register per SM and thread block can be quries using `regsPerMultiprocessor` and `regsPerBlock`. 
- Number of registers used by kernel can be modifed using `maxregcount`. But, using a low number may result in register spilling. 
` **Register Spilling** is when a kernel doesn't have enough register to so it moves ther required variables to local memory instead (over 100x slower than registers). Essentially resulting in a silent reduce in efficiency of the program. 


### Local Memory
Physically utilize the same space as Shared global memory space. But, it's the local storage of the thread (similar to register but slower). 

### Constant Memory
- Grid scope, accessible for entire lifetime of application. Resides on device, is read-only to the kernel. 

Eg:
```cpp
__constant__ float coeffs[4];

__global__ void compute(float* out){
    int idx = threadIdx.x;
    out[idx] = coeffs[0] * idx + coeffs[1];
}

// In host code
float h_coeffs[4] = {1.0f, 2.0f, 3.0f, 4.0f};
cudaMemcpyToSymbol(coeffs, h_coeffs, sizeof(h_coeffs));
compute<<<1, 10>>>(device_out);
```


### Caches
- L1 and L2 caches. 
- L1 and shared memory shared same physical space located in SM. 
- Size can be queried, and behaviour can be customized by a developer. 

### Texture and Surface Memory (No real advantage of using these)
In older CUDA code, texture and surface memory may be used as it provided better performance. 

