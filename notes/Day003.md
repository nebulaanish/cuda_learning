# Day 3

## Programming GPUs in CUDA
https://docs.nvidia.com/cuda/cuda-programming-guide/02-basics/intro-to-cuda-cpp.html


### Kernel 
- Code is specified using `__global__` declaration specifier. 
- Kernel Launch is a operation, that starts a kernel running. A kernel has a void return type. 

```
__global__ void vecAdd(float* A, float* B, float* C){

}

```
- The number of threads that a kernel will execute is specified during kernel launch. 
- The execution configuration during kernel launch is specified under triple chevron (`<<< >>>`)

eg: 
```
__global__ void vecAdd(float* A, float* B, float* C){

}

int main(){
    ...
    // Kernel Invocation
    vecAdd<<<1, 256>>>(A,B,C);
    ...

    // first_param = grid dimension, second_param = thread block dimension. 
}
```

### Asynchronous Kernel Launches
- Kernel launches are asynchronous and the host doesn't wait for the end or even start of kernel in the GPU. So, some form of synchronization is required. 
- The most basic form being, synchronizing the entire GPU. 

- When using 2 or 3 dimensional grids or thread blocks, `dim3` is used as the dimension parameter. 

```
int main(){
    ...
    dim3 grid(16,16);
    dim3 block(8,8);
    MatAdd<<<grid, block>>>(A,B,C);
    ...
}
```

## Thread and Grid Index Intrinsics
1. `gridDim` : Dimension of the grid, as specified in execution configuration. 
2. `blockDim` : Dimension of the thread block, as specified in execution configuration. 
3. `threadIdx`: gives the index of thread, within it's thread block. Each thread in a thread block has different index. 
4. `blockIdx` : Gives the index of the thread block within the grid. Each thread block will have a different index. 

Each of these intrinsics have a `.x`, `.y` and `.z` member. Dimension not specified by launch configuration is set to 1. `threadIdx` and `blockIdx` are zero indexed. 
- `threadIdx.x` will take values from 0 to `blockDim.x-1`. and so on for other dimensions. 
- `blockIdx.x` will take values from 0 to `gridDim.x-1`. 


```
__global__ void vecAdd(float* A, float* B, float* C){
    int workIndex = threadIdx.x + blockDim.x * blockIdx.x;

    C[workIndex] = A[workIndex] + B[workIndex];
}

int main(){
    ...
    // A,B,C are vectors with 1024 elements. 
    vecAdd<<<4, 256>>>(A,B,C);
    ...
}
```

Here, 4 blocks of 256 threads each are used to calcuate. Each calculation happens at a unique workIndex. 

- **First thread block**:  blockIdx.x = 0, so => workIndex = threadIdx.x
- **Second thread block**: blockIdx.x =1, so => workIndex = threadIdx.x + 256*1 
- **Third thread block**:  blockIdx.x = 2, so => workIndex = threadIdx.x + 256*2 

