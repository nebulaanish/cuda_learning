# Day 2

- Successfully installed resolved cuda installation issues in local setup. 

## Cuda Programming model. 
https://docs.nvidia.com/cuda/cuda-programming-guide/01-introduction/programming-model.html#heterogeneous-systems


**Device Code**: Code that executes on GPU. 
- Kernel: The function that is invoked for execution in GPU. 
- Launching the kernel: Act of starting a kernel 

- Similar to launching multiple threads in CPU, is launching kernel in GPU. 

- A GPU -> Collection of GPC(Graphics Processing Cluster)s connected to the GPU memory - Collection of SM(Streaming Multiprocessors) -> 

- A kernel execution spawns millions of threads. Threads are organized into blocks called thread blocks which are organized in grid. 

- Threads within a thread block all have access to the on-chip shared memory, which can be used for exchanging information between threads of a thread block.
- Cuda programming requires that it is possible to execute thread blocks in any order, in parallel or series.

- Thread Block Clusters(Optional Grouping):  Are a group of thread blocks, which together form a grid. 
Can be laid out in 1,2,3 dimensions similar to thread block and grid. 

- Same cluster, processed simulataneously in different SMs within the same GPC. 

## Warps and SIMT
The threads in a thread block are organized into a group of 32 threads called warps. 
A warp executes the kernel in SIMT (Single Instruction Multiple Threads) paradigm. 

All threads in the warp execute the same instruction simultaneously. If threads within a warp, don't follow a control flow branch in execution, they are masked off
while others are executed. 


Thread blocks are best specified to be in multiples of 32. Otherwise, the final warp will have some lanes unused for most of execution resulting in sub-optimal functional unit utilization. 

SIMT(Single Instruction Multiple Threads) is often compared with SIMD(Single Instruction Multiple Data) model of parallelism. 


!! Understanding warp execution model is helpful in understanding: Global Memory Coalescing and Share Memory Bank Access Patterns



## GPU Memory and DRAM in heterogeneous systems. 
DRAM attached to GPU is called global memory as it's accessible to all SMs. 
DRAM atached to CPU is called system or host memory. 

- heterogeneous systems use virutal memory addressing. Both CPU and GPU use a single unified memory space. So given a memory address, it's 
possible to determine which GPU or CPU the address is associated with. 


## On-Chip Memory in GPUS
- In addition to global memory, each SM has their own register file, shared memory and L1 cache.  
- Quick access to threads on same SM but inaccesssible to other threads. 
- Register allocation is per thread, while shared memory is common to each thread block. 

## Caches
- Each SM has their own L1 cache
- A larger L2 cache is shared by all SMs in a GPU. 


## Unified Memory
- Allocated memory is accessible to that device only. 
- By using unified memory, CUDA relocates or enables access. 
- Even with unified memory, accessing from memory where it resides provides optimal performance. 

- **CUDA Dynamic Parallelism** => A thread block may be suspended to memory. 
- **Mapped Memory** => CPU memory allocated with properties that enable direct access from GPU. 



# Cuda Platform
PTX(Parallel Thread Execution): High level assembly language for NVIDIA GPUs. 
NVCC(NVIDIA CUDA Compiler)


## Cubins and Fatbins
- C++ code is compiled to PTX which is then converted to binary code for execution which is CUDA binary or cubin.  
- A cubin has specific binary format for a specific SM version eg: sm_120. 
- GPU code is stored in fatbin. Fatbins contains cubin and PTX for multiple versions of SMs.  
