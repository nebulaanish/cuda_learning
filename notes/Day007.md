# Day 7

### Distributed Shared memory
- A paritioned shared memory, to share memory between threads blocks in a same thread cluster, compute capability 9.0 facilitate by Cooperative groups. 
- Size: number of thread blocks per cluster multiplied by size of the shared memory per thread block. 
- Accessing data from distributed shared memory requires all thread blocks to exists to, thread blocks have started executing can be determined by using `cluster.sync()`. 
- Also, the operations on distributed shared memory should happen before exit of a thread block. 
- Also if thead block A is trying to read B's shared memory. It must complete before B can exit. 


#### Computation of Histogram bins: 
- Standard way of computing histograms is to perform the computation in the shared memory of each thread block and then perform the global memory atomics. 
- But, when the histogram bins no longer fit in the shared memory, as user needs to directly compute histograms and hence the atomics in the global memory. 
- This slows down the compute. 
- With Distributed shared memory, the histogram bins can be computed in shared memory, distributed shared memroy or global memory directly. 

For code refer to `code/007_histogram.cu`

- Learnt by writing code for 1D convolution in cuda. 

