# Day 9
Example of coalesced memory access in Matrix multiplication: 
- Let's assume two 4x4 matrices (a and b). Then, it's values are stored in row-major form. 
- i.e like so: `a11, a12, a13, a14 | a21, a22, a23, a24 | a31, a32, a33, a34 | a41, a42, a43, a44 (same for b)`
- During multiplication: If we do: 
    row * column, then 
- `c11 = a11*b11 + a12*b21 + a13*b31 + a14*b41`
- `c12 = a11*b12 + a12*b22 + a13*b32 + a14*b42`
and so on
-  The threads are assigned in following way: 
```
Thread 0 -> C(row1, col1) -> asks for a11 which is in memory index 0 and b11(in index 0)
Thread 1 -> C(row2, col2) -> asks for a21 which is in memory index 4 and b11(in index 4)
```
Which is not coalesced.

But, if we flip that and assign threads in following pattern. <br>
```
Thread 0 -> (row1, col1)
Thread 1 -> (row2, col1)
```
The memory can be fetched in single DRAM burst or a single memory transaction. 

### Bank Conflicts ... 
- Shared memory is split into 32 banks where each bank may include multiple memory addresses. Warps have same number of threads.  
- Each all 32 threads, access different banks, then speed is fast. 
- Otherwise, it's slow. 
Eg: 32 customers, 32 cashiers. If all 32 customers, go to different cashiers, each one of them get's their work done fast. While, in other cases, 1 cashier may have to serve multiple customers. Resulting in bank conflict, slowing down performance. 

- **Broadcast**: If, multiple threads read the same address, GPU uses broadcast, so no conflict occurs. 

- **Example to study further**: Use of row padding in matrix transpose, to present bank conflict. 

====================================
- Write relu code: `code/009_relu.cu`. 
- Write vector addition: `code/009_vec_add.cu`. 
- Write matrix multiplication: `code/009_mat_mul.cu`. 

